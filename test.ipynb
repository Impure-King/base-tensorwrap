{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorwrap import nn\n",
    "import tensorwrap as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.layers.Layer):\n",
    "    def __init__(self, units) -> None:\n",
    "        super().__init__() # Needed for making it JIT compatible.\n",
    "        self.units = units # Defining the output shape\n",
    "  \n",
    "    def build(self, input_shape: tuple) -> None:\n",
    "        input_shape = tf.shape(input_shape) # Getting appropriate input shape\n",
    "        self.kernel = self.add_weights([input_shape, self.units],\n",
    "                                       initializer = 'glorot_uniform')\n",
    "        self.bias = self.add_weights([self.units],\n",
    "                                     initializer = 'zeros')\n",
    "        super().build(self.kernel, self.bias) # Needed to add the kernel to model.\n",
    "    \n",
    "    # Use call not __call__ to define the flow. No tf.function needed either.\n",
    "    def call(self, inputs):\n",
    "        return inputs @ self.kernel + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.nn.Sequential([\n",
    "    Linear(10),\n",
    "    tf.nn.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(1, int(1e5))\n",
    "x = tf.expand_dims(x, axis=1)\n",
    "x /= 1e5\n",
    "y = x + 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.nn.losses.mse\n",
    "optimizer = tf.nn.optimizers.gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_loop():\n",
    "    global x, y, model\n",
    "    epochs = 100\n",
    "    for epoch in tf.range(epochs):\n",
    "        y_pred = model(x)\n",
    "        training(y_pred, y)\n",
    "\n",
    "# @tf.function\n",
    "def training(y_pred, y):\n",
    "    global model\n",
    "    grads = tf.grad(loss)(y, y_pred)\n",
    "    model.layers = optimizer.apply_gradients(grads, model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>, 10).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_line_magic(\u001b[39m'\u001b[39;49m\u001b[39mtimeit\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtraining_loop()\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2364\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2362\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mlocal_ns\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2363\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2364\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2365\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/IPython/core/magics/execution.py:1162\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m10\u001b[39m):\n\u001b[1;32m   1161\u001b[0m     number \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m index\n\u001b[0;32m-> 1162\u001b[0m     time_number \u001b[39m=\u001b[39m timer\u001b[39m.\u001b[39;49mtimeit(number)\n\u001b[1;32m   1163\u001b[0m     \u001b[39mif\u001b[39;00m time_number \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m:\n\u001b[1;32m   1164\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/IPython/core/magics/execution.py:156\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    154\u001b[0m gc\u001b[39m.\u001b[39mdisable()\n\u001b[1;32m    155\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     timing \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(it, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimer)\n\u001b[1;32m    157\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m gcold:\n",
      "File \u001b[0;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[0;32m~/GitHub/base-tensorwrap/tensorwrap/experimental/wrappers.py:8\u001b[0m, in \u001b[0;36mfunction.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[1;32m      7\u001b[0m     funct \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mjit(func, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mreturn\u001b[39;00m funct(\u001b[39m*\u001b[39;49margs)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mrange(epochs):\n\u001b[0;32m----> 6\u001b[0m     y_pred \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m      7\u001b[0m     training(y_pred, y)\n",
      "File \u001b[0;32m~/GitHub/base-tensorwrap/tensorwrap/nn/models.py:23\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[1;32m     22\u001b[0m     inputs \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall(inputs)\n\u001b[1;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/GitHub/base-tensorwrap/tensorwrap/nn/models.py:123\u001b[0m, in \u001b[0;36mSequential.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, x) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[1;32m    122\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 123\u001b[0m         x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/GitHub/base-tensorwrap/tensorwrap/nn/layers.py:51\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[1;32m     49\u001b[0m     \u001b[39m# This is to compile, in not built.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[0;32m---> 51\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild(tf\u001b[39m.\u001b[39;49mshape(inputs))\n\u001b[1;32m     52\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall(inputs)\n\u001b[1;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m, in \u001b[0;36mLinear.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m, input_shape: \u001b[39mtuple\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     input_shape \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mshape(input_shape) \u001b[39m# Getting appropriate input shape\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_weights([input_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munits],\n\u001b[1;32m      9\u001b[0m                                    initializer \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mglorot_uniform\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_weights([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits],\n\u001b[1;32m     11\u001b[0m                                  initializer \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mbuild(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/GitHub/base-tensorwrap/tensorwrap/nn/layers.py:40\u001b[0m, in \u001b[0;36mLayer.add_weights\u001b[0;34m(self, shape, initializer, trainable, name)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39melif\u001b[39;00m initializer \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mglorot_uniform\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     39\u001b[0m     key \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mPRNGKey(randint(\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m jax\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49muniform(key, shape, dtype \u001b[39m=\u001b[39;49m tf\u001b[39m.\u001b[39;49mfloat32)\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/_src/random.py:264\u001b[0m, in \u001b[0;36muniform\u001b[0;34m(key, shape, dtype, minval, maxval)\u001b[0m\n\u001b[1;32m    261\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdtype argument to `uniform` must be a float dtype, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mcanonicalize_dtype(dtype)\n\u001b[0;32m--> 264\u001b[0m shape \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39;49mas_named_shape(shape)\n\u001b[1;32m    265\u001b[0m \u001b[39mreturn\u001b[39;00m _uniform(key, shape, dtype, minval, maxval)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/core.py:1904\u001b[0m, in \u001b[0;36mcanonicalize_shape\u001b[0;34m(shape, context)\u001b[0m\n\u001b[1;32m   1902\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   1903\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[0;32m-> 1904\u001b[0m \u001b[39mraise\u001b[39;00m _invalid_shape_error(shape, context)\n",
      "\u001b[0;31mTypeError\u001b[0m: Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>, 10).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions."
     ]
    }
   ],
   "source": [
    "%timeit training_loop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prebuilt training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 ms ± 1.74 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.nn.losses.mse,\n",
    "    optimizer=tf.nn.optimizers.gradient_descent(),\n",
    "    metrics=tf.nn.losses.mae\n",
    ")\n",
    "%timeit model.fit(x, x + 10, epochs=100, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorWrap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
