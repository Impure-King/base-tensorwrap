{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.0.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorwrap as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 2000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_train \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mrange(start \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, end \u001b[39m=\u001b[39;49m \u001b[39m1000\u001b[39;49m, delta \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m y_train \u001b[39m=\u001b[39m x_train \u001b[39m+\u001b[39m \u001b[39m10\u001b[39m\n",
      "File \u001b[0;32m~/GitHub/base-tensorwrap/tensorwrap/core/tensors.py:27\u001b[0m, in \u001b[0;36mrange\u001b[0;34m(start, end, delta, dtype)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrange\u001b[39m(start, end, delta \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, dtype \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     23\u001b[0m     \u001b[39m\"\"\"Creates a range of values in an array.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m    Start: the value to start\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m    End: the value to end before.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m    Delta: the common difference of the sequence\"\"\"\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     array \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39;49marange(start, end, step \u001b[39m=\u001b[39;49m delta, dtype\u001b[39m=\u001b[39;49m dtype)\n\u001b[1;32m     28\u001b[0m     \u001b[39mreturn\u001b[39;00m array\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:2267\u001b[0m, in \u001b[0;36marange\u001b[0;34m(start, stop, step, dtype)\u001b[0m\n\u001b[1;32m   2265\u001b[0m   stop \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mceil(stop)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n\u001b[1;32m   2266\u001b[0m   \u001b[39mreturn\u001b[39;00m lax\u001b[39m.\u001b[39miota(dtype, stop)\n\u001b[0;32m-> 2267\u001b[0m \u001b[39mreturn\u001b[39;00m array(np\u001b[39m.\u001b[39;49marange(start, stop\u001b[39m=\u001b[39;49mstop, step\u001b[39m=\u001b[39;49mstep, dtype\u001b[39m=\u001b[39;49mdtype))\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:2017\u001b[0m, in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   2013\u001b[0m     \u001b[39mreturn\u001b[39;00m array(np\u001b[39m.\u001b[39masarray(view), dtype, copy, ndmin\u001b[39m=\u001b[39mndmin)\n\u001b[1;32m   2015\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected input type for array: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mobject\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2017\u001b[0m out_array: Array \u001b[39m=\u001b[39m lax_internal\u001b[39m.\u001b[39;49m_convert_element_type(out, dtype, weak_type\u001b[39m=\u001b[39;49mweak_type)\n\u001b[1;32m   2018\u001b[0m \u001b[39mif\u001b[39;00m ndmin \u001b[39m>\u001b[39m ndim(out_array):\n\u001b[1;32m   2019\u001b[0m   out_array \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mexpand_dims(out_array, \u001b[39mrange\u001b[39m(ndmin \u001b[39m-\u001b[39m ndim(out_array)))\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/_src/lax/lax.py:588\u001b[0m, in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    586\u001b[0m   \u001b[39mreturn\u001b[39;00m type_cast(Array, operand)\n\u001b[1;32m    587\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 588\u001b[0m   \u001b[39mreturn\u001b[39;00m convert_element_type_p\u001b[39m.\u001b[39;49mbind(operand, new_dtype\u001b[39m=\u001b[39;49mnew_dtype,\n\u001b[1;32m    589\u001b[0m                                      weak_type\u001b[39m=\u001b[39;49m\u001b[39mbool\u001b[39;49m(weak_type))\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/core.py:329\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[1;32m    327\u001b[0m   \u001b[39massert\u001b[39;00m (\u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mjax_enable_checks \u001b[39mor\u001b[39;00m\n\u001b[1;32m    328\u001b[0m           \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(arg, Tracer) \u001b[39mor\u001b[39;00m valid_jaxtype(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)), args\n\u001b[0;32m--> 329\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/core.py:332\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind_with_trace\u001b[39m(\u001b[39mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 332\u001b[0m   out \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39;49mprocess_primitive(\u001b[39mself\u001b[39;49m, \u001b[39mmap\u001b[39;49m(trace\u001b[39m.\u001b[39;49mfull_raise, args), params)\n\u001b[1;32m    333\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39m(full_lower, out) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/core.py:712\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_primitive\u001b[39m(\u001b[39mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 712\u001b[0m   \u001b[39mreturn\u001b[39;00m primitive\u001b[39m.\u001b[39;49mimpl(\u001b[39m*\u001b[39;49mtracers, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/_src/dispatch.py:120\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m compiled_fun \u001b[39m=\u001b[39m xla_primitive_callable(prim, \u001b[39m*\u001b[39munsafe_map(arg_spec, args),\n\u001b[1;32m    119\u001b[0m                                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fun(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/_src/dispatch.py:205\u001b[0m, in \u001b[0;36mxla_primitive_callable.<locals>.<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    202\u001b[0m compiled \u001b[39m=\u001b[39m _xla_callable_uncached(lu\u001b[39m.\u001b[39mwrap_init(prim_fun), device, \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    203\u001b[0m                                   prim\u001b[39m.\u001b[39mname, donated_invars, \u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39marg_specs)\n\u001b[1;32m    204\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m prim\u001b[39m.\u001b[39mmultiple_results:\n\u001b[0;32m--> 205\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: compiled(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[39mreturn\u001b[39;00m compiled\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/interpreters/pxla.py:3638\u001b[0m, in \u001b[0;36m_execute_trivial\u001b[0;34m(jaxpr, consts, in_handler, out_handler, kept_var_idx, *args)\u001b[0m\n\u001b[1;32m   3635\u001b[0m \u001b[39mmap\u001b[39m(env\u001b[39m.\u001b[39msetdefault, jaxpr\u001b[39m.\u001b[39mconstvars, consts)\n\u001b[1;32m   3636\u001b[0m outs \u001b[39m=\u001b[39m [xla\u001b[39m.\u001b[39mcanonicalize_dtype(v\u001b[39m.\u001b[39mval) \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(v) \u001b[39mis\u001b[39;00m core\u001b[39m.\u001b[39mLiteral \u001b[39melse\u001b[39;00m env[v]\n\u001b[1;32m   3637\u001b[0m         \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m jaxpr\u001b[39m.\u001b[39moutvars]\n\u001b[0;32m-> 3638\u001b[0m \u001b[39mreturn\u001b[39;00m out_handler(in_handler(outs))\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/interpreters/pxla.py:1895\u001b[0m, in \u001b[0;36mInputsHandler.__call__\u001b[0;34m(self, input_buffers)\u001b[0m\n\u001b[1;32m   1894\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, input_buffers):\n\u001b[0;32m-> 1895\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandler(input_buffers)\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[39mwith\u001b[39;00m TraceAnnotation(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/interpreters/pxla.py:416\u001b[0m, in \u001b[0;36mshard_args\u001b[0;34m(devices, indices, mode, args)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[39m@profiler\u001b[39m\u001b[39m.\u001b[39mannotate_function\n\u001b[1;32m    398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshard_args\u001b[39m(devices: Sequence[xb\u001b[39m.\u001b[39mxla_client\u001b[39m.\u001b[39mDevice],\n\u001b[1;32m    399\u001b[0m                indices: Sequence[Sequence[Index]],\n\u001b[1;32m    400\u001b[0m                mode: InputsHandlerMode,\n\u001b[1;32m    401\u001b[0m                args) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Sequence[Union[xb\u001b[39m.\u001b[39mShardedBuffer, Sequence[xb\u001b[39m.\u001b[39mxla_client\u001b[39m.\u001b[39mBuffer]]]:\n\u001b[1;32m    402\u001b[0m   \u001b[39m\"\"\"Shard each argument data array along its leading axis.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \n\u001b[1;32m    404\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39m    for each argument.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m   \u001b[39mreturn\u001b[39;00m [_shard_arg(arg, devices, indices[i], mode) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(args)]\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/interpreters/pxla.py:416\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[39m@profiler\u001b[39m\u001b[39m.\u001b[39mannotate_function\n\u001b[1;32m    398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshard_args\u001b[39m(devices: Sequence[xb\u001b[39m.\u001b[39mxla_client\u001b[39m.\u001b[39mDevice],\n\u001b[1;32m    399\u001b[0m                indices: Sequence[Sequence[Index]],\n\u001b[1;32m    400\u001b[0m                mode: InputsHandlerMode,\n\u001b[1;32m    401\u001b[0m                args) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Sequence[Union[xb\u001b[39m.\u001b[39mShardedBuffer, Sequence[xb\u001b[39m.\u001b[39mxla_client\u001b[39m.\u001b[39mBuffer]]]:\n\u001b[1;32m    402\u001b[0m   \u001b[39m\"\"\"Shard each argument data array along its leading axis.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \n\u001b[1;32m    404\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[39m    for each argument.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m   \u001b[39mreturn\u001b[39;00m [_shard_arg(arg, devices, indices[i], mode) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(args)]\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/interpreters/pxla.py:394\u001b[0m, in \u001b[0;36m_shard_arg\u001b[0;34m(arg, devices, arg_indices, mode)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m   arg \u001b[39m=\u001b[39m xla\u001b[39m.\u001b[39mcanonicalize_dtype(arg)\n\u001b[0;32m--> 394\u001b[0m   \u001b[39mreturn\u001b[39;00m shard_arg_handlers[\u001b[39mtype\u001b[39;49m(arg)](arg, devices, arg_indices, mode)\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/interpreters/pxla.py:428\u001b[0m, in \u001b[0;36m_shard_array\u001b[0;34m(x, devices, indices, mode)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mfloat0:\n\u001b[1;32m    427\u001b[0m   x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(x\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mdtype(\u001b[39mbool\u001b[39m))\n\u001b[0;32m--> 428\u001b[0m \u001b[39mreturn\u001b[39;00m device_put([x[i] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m indices], devices)\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/interpreters/pxla.py:3916\u001b[0m, in \u001b[0;36mdevice_put\u001b[0;34m(x, devices, replicate)\u001b[0m\n\u001b[1;32m   3914\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(it\u001b[39m.\u001b[39mchain\u001b[39m.\u001b[39mfrom_iterable(dispatch\u001b[39m.\u001b[39mdevice_put(x, device) \u001b[39mfor\u001b[39;00m device \u001b[39min\u001b[39;00m devices))\n\u001b[1;32m   3915\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3916\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(it\u001b[39m.\u001b[39;49mchain\u001b[39m.\u001b[39;49mfrom_iterable(dispatch\u001b[39m.\u001b[39;49mdevice_put(val, device) \u001b[39mfor\u001b[39;49;00m val, device \u001b[39min\u001b[39;49;00m safe_zip(x, devices)))\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/interpreters/pxla.py:3916\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3914\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(it\u001b[39m.\u001b[39mchain\u001b[39m.\u001b[39mfrom_iterable(dispatch\u001b[39m.\u001b[39mdevice_put(x, device) \u001b[39mfor\u001b[39;00m device \u001b[39min\u001b[39;00m devices))\n\u001b[1;32m   3915\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3916\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(it\u001b[39m.\u001b[39mchain\u001b[39m.\u001b[39mfrom_iterable(dispatch\u001b[39m.\u001b[39;49mdevice_put(val, device) \u001b[39mfor\u001b[39;00m val, device \u001b[39min\u001b[39;00m safe_zip(x, devices)))\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/_src/dispatch.py:1266\u001b[0m, in \u001b[0;36mdevice_put\u001b[0;34m(x, device)\u001b[0m\n\u001b[1;32m   1264\u001b[0m x \u001b[39m=\u001b[39m xla\u001b[39m.\u001b[39mcanonicalize_dtype(x)\n\u001b[1;32m   1265\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1266\u001b[0m   \u001b[39mreturn\u001b[39;00m device_put_handlers[\u001b[39mtype\u001b[39;49m(x)](x, device)\n\u001b[1;32m   1267\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m   1268\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo device_put handler for type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(x)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/TensorWrap/lib/python3.10/site-packages/jax/_src/dispatch.py:1277\u001b[0m, in \u001b[0;36m_device_put_array\u001b[0;34m(x, device)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mfloat0:\n\u001b[1;32m   1276\u001b[0m   x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(x\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mdtype(\u001b[39mbool\u001b[39m))\n\u001b[0;32m-> 1277\u001b[0m \u001b[39mreturn\u001b[39;00m (backend\u001b[39m.\u001b[39;49mbuffer_from_pyval(x, device),)\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 2000 bytes."
     ]
    }
   ],
   "source": [
    "x_train = tf.range(start = 1, end = 1000, delta = 2)\n",
    "y_train = x_train + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([  11.,   13.,   15.,   17.,   19.,   21.,   23.,   25.,   27.,\n",
       "         29.,   31.,   33.,   35.,   37.,   39.,   41.,   43.,   45.,\n",
       "         47.,   49.,   51.,   53.,   55.,   57.,   59.,   61.,   63.,\n",
       "         65.,   67.,   69.,   71.,   73.,   75.,   77.,   79.,   81.,\n",
       "         83.,   85.,   87.,   89.,   91.,   93.,   95.,   97.,   99.,\n",
       "        101.,  103.,  105.,  107.,  109.,  111.,  113.,  115.,  117.,\n",
       "        119.,  121.,  123.,  125.,  127.,  129.,  131.,  133.,  135.,\n",
       "        137.,  139.,  141.,  143.,  145.,  147.,  149.,  151.,  153.,\n",
       "        155.,  157.,  159.,  161.,  163.,  165.,  167.,  169.,  171.,\n",
       "        173.,  175.,  177.,  179.,  181.,  183.,  185.,  187.,  189.,\n",
       "        191.,  193.,  195.,  197.,  199.,  201.,  203.,  205.,  207.,\n",
       "        209.,  211.,  213.,  215.,  217.,  219.,  221.,  223.,  225.,\n",
       "        227.,  229.,  231.,  233.,  235.,  237.,  239.,  241.,  243.,\n",
       "        245.,  247.,  249.,  251.,  253.,  255.,  257.,  259.,  261.,\n",
       "        263.,  265.,  267.,  269.,  271.,  273.,  275.,  277.,  279.,\n",
       "        281.,  283.,  285.,  287.,  289.,  291.,  293.,  295.,  297.,\n",
       "        299.,  301.,  303.,  305.,  307.,  309.,  311.,  313.,  315.,\n",
       "        317.,  319.,  321.,  323.,  325.,  327.,  329.,  331.,  333.,\n",
       "        335.,  337.,  339.,  341.,  343.,  345.,  347.,  349.,  351.,\n",
       "        353.,  355.,  357.,  359.,  361.,  363.,  365.,  367.,  369.,\n",
       "        371.,  373.,  375.,  377.,  379.,  381.,  383.,  385.,  387.,\n",
       "        389.,  391.,  393.,  395.,  397.,  399.,  401.,  403.,  405.,\n",
       "        407.,  409.,  411.,  413.,  415.,  417.,  419.,  421.,  423.,\n",
       "        425.,  427.,  429.,  431.,  433.,  435.,  437.,  439.,  441.,\n",
       "        443.,  445.,  447.,  449.,  451.,  453.,  455.,  457.,  459.,\n",
       "        461.,  463.,  465.,  467.,  469.,  471.,  473.,  475.,  477.,\n",
       "        479.,  481.,  483.,  485.,  487.,  489.,  491.,  493.,  495.,\n",
       "        497.,  499.,  501.,  503.,  505.,  507.,  509.,  511.,  513.,\n",
       "        515.,  517.,  519.,  521.,  523.,  525.,  527.,  529.,  531.,\n",
       "        533.,  535.,  537.,  539.,  541.,  543.,  545.,  547.,  549.,\n",
       "        551.,  553.,  555.,  557.,  559.,  561.,  563.,  565.,  567.,\n",
       "        569.,  571.,  573.,  575.,  577.,  579.,  581.,  583.,  585.,\n",
       "        587.,  589.,  591.,  593.,  595.,  597.,  599.,  601.,  603.,\n",
       "        605.,  607.,  609.,  611.,  613.,  615.,  617.,  619.,  621.,\n",
       "        623.,  625.,  627.,  629.,  631.,  633.,  635.,  637.,  639.,\n",
       "        641.,  643.,  645.,  647.,  649.,  651.,  653.,  655.,  657.,\n",
       "        659.,  661.,  663.,  665.,  667.,  669.,  671.,  673.,  675.,\n",
       "        677.,  679.,  681.,  683.,  685.,  687.,  689.,  691.,  693.,\n",
       "        695.,  697.,  699.,  701.,  703.,  705.,  707.,  709.,  711.,\n",
       "        713.,  715.,  717.,  719.,  721.,  723.,  725.,  727.,  729.,\n",
       "        731.,  733.,  735.,  737.,  739.,  741.,  743.,  745.,  747.,\n",
       "        749.,  751.,  753.,  755.,  757.,  759.,  761.,  763.,  765.,\n",
       "        767.,  769.,  771.,  773.,  775.,  777.,  779.,  781.,  783.,\n",
       "        785.,  787.,  789.,  791.,  793.,  795.,  797.,  799.,  801.,\n",
       "        803.,  805.,  807.,  809.,  811.,  813.,  815.,  817.,  819.,\n",
       "        821.,  823.,  825.,  827.,  829.,  831.,  833.,  835.,  837.,\n",
       "        839.,  841.,  843.,  845.,  847.,  849.,  851.,  853.,  855.,\n",
       "        857.,  859.,  861.,  863.,  865.,  867.,  869.,  871.,  873.,\n",
       "        875.,  877.,  879.,  881.,  883.,  885.,  887.,  889.,  891.,\n",
       "        893.,  895.,  897.,  899.,  901.,  903.,  905.,  907.,  909.,\n",
       "        911.,  913.,  915.,  917.,  919.,  921.,  923.,  925.,  927.,\n",
       "        929.,  931.,  933.,  935.,  937.,  939.,  941.,  943.,  945.,\n",
       "        947.,  949.,  951.,  953.,  955.,  957.,  959.,  961.,  963.,\n",
       "        965.,  967.,  969.,  971.,  973.,  975.,  977.,  979.,  981.,\n",
       "        983.,  985.,  987.,  989.,  991.,  993.,  995.,  997.,  999.,\n",
       "       1001., 1003., 1005., 1007., 1009.], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding the axis:\n",
    "x_train = tf.expand_dims(x_train, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.nn.layers.Dense(units = 1, input_shape = [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.nn.Sequential([\n",
    "    layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model:\n",
    "model.compile(\n",
    "    loss = tf.nn.losses.MSE,\n",
    "    optimizer = tf.nn.optimizers.SGD(learning_rate = 0.1),\n",
    "    accuracy = tf.nn.losses.MAE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Loss = 509.8795471191406\n",
      "Epoch 1 : Loss = 509.063720703125\n",
      "Epoch 2 : Loss = 508.24920654296875\n",
      "Epoch 3 : Loss = 507.43609619140625\n",
      "Epoch 4 : Loss = 506.62408447265625\n",
      "Epoch 5 : Loss = 505.81353759765625\n",
      "Epoch 6 : Loss = 505.0043029785156\n",
      "Epoch 7 : Loss = 504.1962890625\n",
      "Epoch 8 : Loss = 503.38958740234375\n",
      "Epoch 9 : Loss = 502.5840759277344\n",
      "Epoch 10 : Loss = 501.77996826171875\n",
      "Epoch 11 : Loss = 500.9770812988281\n",
      "Epoch 12 : Loss = 500.1756286621094\n",
      "Epoch 13 : Loss = 499.3752136230469\n",
      "Epoch 14 : Loss = 498.5762634277344\n",
      "Epoch 15 : Loss = 497.7785339355469\n",
      "Epoch 16 : Loss = 496.9821472167969\n",
      "Epoch 17 : Loss = 496.18695068359375\n",
      "Epoch 18 : Loss = 495.3930358886719\n",
      "Epoch 19 : Loss = 494.6004638671875\n",
      "Epoch 20 : Loss = 493.8090515136719\n",
      "Epoch 21 : Loss = 493.01898193359375\n",
      "Epoch 22 : Loss = 492.2301025390625\n",
      "Epoch 23 : Loss = 491.44256591796875\n",
      "Epoch 24 : Loss = 490.6562805175781\n",
      "Epoch 25 : Loss = 489.8711853027344\n",
      "Epoch 26 : Loss = 489.08740234375\n",
      "Epoch 27 : Loss = 488.30487060546875\n",
      "Epoch 28 : Loss = 487.5235290527344\n",
      "Epoch 29 : Loss = 486.74359130859375\n",
      "Epoch 30 : Loss = 485.9648132324219\n",
      "Epoch 31 : Loss = 485.1872253417969\n",
      "Epoch 32 : Loss = 484.41094970703125\n",
      "Epoch 33 : Loss = 483.6358337402344\n",
      "Epoch 34 : Loss = 482.86199951171875\n",
      "Epoch 35 : Loss = 482.08941650390625\n",
      "Epoch 36 : Loss = 481.3182067871094\n",
      "Epoch 37 : Loss = 480.5479736328125\n",
      "Epoch 38 : Loss = 479.7791442871094\n",
      "Epoch 39 : Loss = 479.0115661621094\n",
      "Epoch 40 : Loss = 478.2451171875\n",
      "Epoch 41 : Loss = 477.47991943359375\n",
      "Epoch 42 : Loss = 476.71588134765625\n",
      "Epoch 43 : Loss = 475.9531555175781\n",
      "Epoch 44 : Loss = 475.1916198730469\n",
      "Epoch 45 : Loss = 474.4312744140625\n",
      "Epoch 46 : Loss = 473.6722717285156\n",
      "Epoch 47 : Loss = 472.9143981933594\n",
      "Epoch 48 : Loss = 472.15771484375\n",
      "Epoch 49 : Loss = 471.4023132324219\n",
      "Epoch 50 : Loss = 470.64801025390625\n",
      "Epoch 51 : Loss = 469.8949890136719\n",
      "Epoch 52 : Loss = 469.14312744140625\n",
      "Epoch 53 : Loss = 468.39251708984375\n",
      "Epoch 54 : Loss = 467.6430969238281\n",
      "Epoch 55 : Loss = 466.8948669433594\n",
      "Epoch 56 : Loss = 466.14788818359375\n",
      "Epoch 57 : Loss = 465.40191650390625\n",
      "Epoch 58 : Loss = 464.6573486328125\n",
      "Epoch 59 : Loss = 463.9139404296875\n",
      "Epoch 60 : Loss = 463.1717224121094\n",
      "Epoch 61 : Loss = 462.4305725097656\n",
      "Epoch 62 : Loss = 461.6906433105469\n",
      "Epoch 63 : Loss = 460.9520263671875\n",
      "Epoch 64 : Loss = 460.21441650390625\n",
      "Epoch 65 : Loss = 459.47808837890625\n",
      "Epoch 66 : Loss = 458.7430114746094\n",
      "Epoch 67 : Loss = 458.0089111328125\n",
      "Epoch 68 : Loss = 457.27618408203125\n",
      "Epoch 69 : Loss = 456.5444641113281\n",
      "Epoch 70 : Loss = 455.8139953613281\n",
      "Epoch 71 : Loss = 455.08477783203125\n",
      "Epoch 72 : Loss = 454.35650634765625\n",
      "Epoch 73 : Loss = 453.6295471191406\n",
      "Epoch 74 : Loss = 452.9038391113281\n",
      "Epoch 75 : Loss = 452.17919921875\n",
      "Epoch 76 : Loss = 451.45562744140625\n",
      "Epoch 77 : Loss = 450.73333740234375\n",
      "Epoch 78 : Loss = 450.0121765136719\n",
      "Epoch 79 : Loss = 449.2920837402344\n",
      "Epoch 80 : Loss = 448.5732727050781\n",
      "Epoch 81 : Loss = 447.85552978515625\n",
      "Epoch 82 : Loss = 447.1390075683594\n",
      "Epoch 83 : Loss = 446.42352294921875\n",
      "Epoch 84 : Loss = 445.7093200683594\n",
      "Epoch 85 : Loss = 444.99615478515625\n",
      "Epoch 86 : Loss = 444.2841796875\n",
      "Epoch 87 : Loss = 443.57330322265625\n",
      "Epoch 88 : Loss = 442.8636169433594\n",
      "Epoch 89 : Loss = 442.1549987792969\n",
      "Epoch 90 : Loss = 441.447509765625\n",
      "Epoch 91 : Loss = 440.7412109375\n",
      "Epoch 92 : Loss = 440.03607177734375\n",
      "Epoch 93 : Loss = 439.33203125\n",
      "Epoch 94 : Loss = 438.6290283203125\n",
      "Epoch 95 : Loss = 437.9272766113281\n",
      "Epoch 96 : Loss = 437.2265930175781\n",
      "Epoch 97 : Loss = 436.5269775390625\n",
      "Epoch 98 : Loss = 435.8285827636719\n",
      "Epoch 99 : Loss = 435.1312561035156\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train = x_train, y_train = y_train, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1890.0881, dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__call__(tf.Variable([99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1, 2, 3, 4, 5], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Variable([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jax.Array"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.jax.Array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorWrap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da53633a286fdc5fa9862d67a3a85a7eec918dfea906e3018d3e6a5fef740b93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
